<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title><![CDATA[Machine Learning]]></title>
        <description><![CDATA[Machine Learning]]></description>
        <link>http://machinelearning.engineering/</link>
        <generator>The Grid</generator>
        <lastBuildDate>Wed, 16 May 2018 16:42:49 GMT</lastBuildDate>
        <atom:link href="http://machinelearning.engineering/rss.xml" rel="self" type="application/rss+xml"/>
        <pubDate>Wed, 16 May 2018 16:42:47 GMT</pubDate>
        <item>
            <title><![CDATA[Lessons from My First Two Years of AI Research]]></title>
            <description><![CDATA[<article><h1>Lessons from My First Two Years of AI Research</h1><p>Tom Silver | About Me | Favorite Papers | Blog By Tom Silver A friend of mine who is about to start a career in artificial intelligence research recently asked what I wish I had known when I started two years ago. Below are some lessons I have learned so far.</p></article>]]></description>
            <link>http://web.mit.edu/tslvr/www/lessons_two_years.html</link>
            <guid isPermaLink="false">84aa9f11-1ca2-40bf-a8fe-72f1bb4690ca</guid>
            <pubDate>Wed, 16 May 2018 16:42:45 GMT</pubDate>
        </item>
        <item>
            <title><![CDATA[Lessons Learned Reproducing a Deep Reinforcement Learning Paper]]></title>
            <description><![CDATA[<article><h1>Lessons Learned Reproducing a Deep Reinforcement Learning Paper</h1><p>There are a lot of neat things going on in deep reinforcement learning. One of the coolest things from last year was OpenAI and DeepMind&apos;s work on training an agent using feedback from a human rather than a classical reward signal.</p><img src="http://amid.fish/images/me.png"></article>]]></description>
            <link>http://amid.fish/reproducing-deep-rl</link>
            <guid isPermaLink="false">e5f8bcd9-1d28-4694-bea0-80b7c16f5f3c</guid>
            <pubDate>Fri, 11 May 2018 07:08:06 GMT</pubDate>
        </item>
        <item>
            <title><![CDATA[Monte Carlo Tree Search - beginners guide - Machine learning blog]]></title>
            <description><![CDATA[<article><h1>Monte Carlo Tree Search - beginners guide - Machine learning blog</h1><p>For quite a long time, a common opinion in academic world was that machine achieving human master performance level in the game of Go was far from realistic. It was considered a &apos;holy grail&apos; of AI - a milestone we were quite far away from reaching within upcoming decade.</p><img src="https://int8.io/wp-content/uploads/2018/03/min-maxtree.png"></article>]]></description>
            <link>https://int8.io/monte-carlo-tree-search-beginners-guide/</link>
            <guid isPermaLink="false">a483be8f-9db6-4e3e-9815-f9ce794e48f6</guid>
            <pubDate>Sun, 06 May 2018 14:15:58 GMT</pubDate>
        </item>
        <item>
            <title><![CDATA[Facebook AI | Tools | Open Source Deep Learning Tools]]></title>
            <description><![CDATA[<article><h1>Facebook AI | Tools | Open Source Deep Learning Tools</h1><p>Experience flexible research and accelerated production with Facebook&apos;s ecosystem of open source, state-of-the-art AI developer tools.</p><img src="https://facebook.ai/wp-content/themes/fb-ai/images/share.jpg"></article>]]></description>
            <link>https://facebook.ai/developers/tools#libraries</link>
            <guid isPermaLink="false">12109d5a-c2b8-49f1-abba-b74adef2c816</guid>
            <pubDate>Thu, 03 May 2018 02:59:15 GMT</pubDate>
        </item>
        <item>
            <title><![CDATA[Grokking Deep Reinforcement Learning]]></title>
            <description><![CDATA[<article><h1>Grokking Deep Reinforcement Learning</h1><p>The must-have book, for anyone that wants to have a profound understanding of deep reinforcement learning.</p><img src="https://images.manning.com/720/960/resize/book/4/35e3f19-ae90-4b03-ad85-9b3dfe700aeb/Morales_DRL_hiresMEAP.png"></article>]]></description>
            <link>https://www.manning.com/books/grokking-deep-reinforcement-learning</link>
            <guid isPermaLink="false">7b04b2a4-fb23-458f-b84e-0be624282aed</guid>
            <pubDate>Thu, 03 May 2018 02:59:13 GMT</pubDate>
        </item>
        <item>
            <title><![CDATA[RemoteML - Remote Machine Learning Jobs: AI, Data Science, Deep Learning.]]></title>
            <description><![CDATA[<article><h1>RemoteML - Remote Machine Learning Jobs: AI, Data Science, Deep Learning.</h1><p>The best candidates are distributed all over the world. Find the best Machine Learning, Deep Learning and Data Science talent all over the world on RemoteML</p><img src="https://remoteml.com/static/img/remoteml-wallpaper.jpg"></article>]]></description>
            <link>https://remoteml.com/</link>
            <guid isPermaLink="false">9fd435ca-783d-4191-8a3f-e133f286ee89</guid>
            <pubDate>Sun, 29 Apr 2018 11:44:10 GMT</pubDate>
        </item>
        <item>
            <title><![CDATA[PyTorch - Internal Architecture Tour]]></title>
            <description><![CDATA[<article><h1>PyTorch - Internal Architecture Tour</h1><p>Introduction Short intro to Python extension objects in C/C++ Zero-copy PyTorch Tensor to Numpy and vice-versa Tensor Storage Shared Memory DLPack: a hope for the Deep Learning frameworks Babel Introduction This post is a tour around the PyTorch codebase, it is meant to be a guide for the architectural design of</p><img src="https://pbs.twimg.com/profile_images/634134295607668736/FdZjcEl2_400x400.jpg"></article>]]></description>
            <link>http://blog.christianperone.com/2018/03/pytorch-internal-architecture-tour/</link>
            <guid isPermaLink="false">fd5cd942-6011-4d93-b909-300a3c3309f8</guid>
            <pubDate>Sun, 29 Apr 2018 11:44:05 GMT</pubDate>
        </item>
        <item>
            <title><![CDATA[Introduction to Recurrent Neural Networks in Pytorch - CPUheater]]></title>
            <description><![CDATA[<article><h1>Introduction to Recurrent Neural Networks in Pytorch - CPUheater</h1><p>This tutorial is intended for someone who wants to understand how Recurrent Neural Network works, no prior knowledge about RNN is required. We will implement the most simple RNN model - Elman Recurrent Neural Network. To get a better understanding of RNNs, we will build it from scratch using Pytorch tensor package and autograd library.</p><img src="https://www.cpuheater.com/wp-content/uploads/2017/12/RNN-logo-7.png"></article>]]></description>
            <link>https://www.cpuheater.com/deep-learning/introduction-to-recurrent-neural-networks-in-pytorch/</link>
            <guid isPermaLink="false">80bf0fb1-0c88-4c37-bf85-a2bbbde8df75</guid>
            <pubDate>Sun, 29 Apr 2018 11:44:02 GMT</pubDate>
        </item>
        <item>
            <title><![CDATA[Embed, encode, attend, predict: The new deep learning formula for state-of-the-art NLP models · Blog · Explosion AI]]></title>
            <description><![CDATA[<article><h1>Embed, encode, attend, predict: The new deep learning formula for state-of-the-art NLP models &middot; Blog &middot; Explosion AI</h1><p>Over the last six months, a powerful new neural network playbook has come together for Natural Language Processing. The new approach can be summarised as a simple four-step formula: embed, encode, attend, predict.</p><img src="https://explosion.ai/blog/img/deep-learning-formula-nlp.jpg"></article>]]></description>
            <link>https://explosion.ai/blog/deep-learning-formula-nlp</link>
            <guid isPermaLink="false">6a3f55bf-39d9-4e7f-9dbc-e854983c5774</guid>
            <pubDate>Mon, 23 Apr 2018 14:31:15 GMT</pubDate>
        </item>
        <item>
            <title><![CDATA[The Building Blocks of Interpretability]]></title>
            <description><![CDATA[<article><h1>The Building Blocks of Interpretability</h1><p>With the growing success of neural networks, there is a corresponding need to be able to explain their decisions&thinsp;-&thinsp;including building con&filig;dence about how they will behave in the real-world, detecting model bias, and for scienti&filig;c curiosity. In order to do so, we need to both construct deep abstractions and reify (or instantiate) them in rich interfaces .</p><img src="https://distill.pub/2018/building-blocks/thumbnail.jpg"></article>]]></description>
            <link>https://distill.pub/2018/building-blocks/</link>
            <guid isPermaLink="false">f27e084e-1fdc-4339-ab5f-6587563e08b4</guid>
            <pubDate>Mon, 02 Apr 2018 05:14:18 GMT</pubDate>
        </item>
    </channel>
</rss>